{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This is a notebook for grabbing all the Github event data that the ecosystem dashboard has for our \"spec and improvement proposal\" repos.\n",
    "\n",
    "This is useful if you want to analyze GitHub actions beyond opening PRs and issues.  It will also show comments, PR/issue closing, etc.\n",
    "\n",
    "It was used as part of the PL EngRes summit to populate the \"Network Native Development\" slide: https://docs.google.com/presentation/d/1dRgEgEpR2htMgyIVXG0fwhBMVwnAsEtXNfvrmzHTqfI/edit#slide=id.g14b7a7f445c_0_476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import ecosystem_dashboard_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_date = datetime.date.today()\n",
    "report_date_str = report_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "from datetime import date\n",
    "analysis_start_date = date(2022, 1, 1) # Adjust for how far back you want to look\n",
    "number_of_days = (report_date - analysis_start_date).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecosystem dashboard URLs are generated based on off of these\n",
    "repo_configs = [\n",
    "    {\n",
    "        \"ecosystem\" : \"ipfs\",\n",
    "        \"org\" : \"ipfs\",\n",
    "        \"repo\" : \"specs\"\n",
    "    }, \n",
    "    {\n",
    "        \"ecosystem\" : \"ipfs\",\n",
    "        \"org\" : \"libp2p\",\n",
    "        \"repo\" : \"specs\"\n",
    "    },\n",
    "    {\n",
    "        \"ecosystem\" : \"filecoin\",\n",
    "        \"org\" : \"filecoin-project\",\n",
    "        \"repo\" : \"FIPs\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"payload\" field add a bunch of data that we don't need so we strip it out to make the json more wieldly to consume\n",
    "def payload_filter(x): \n",
    "    del x[\"payload\"]\n",
    "    return x\n",
    "\n",
    "for repo_config in repo_configs:\n",
    "    ecosystem = repo_config[\"ecosystem\"]\n",
    "    org = repo_config[\"org\"]\n",
    "    repo = repo_config[\"repo\"]\n",
    "    events_path = f\"{org}-{repo}-events-{report_date_str}.json\"\n",
    "    ecosystem_dashboard_utils.dump_api(unpaginated_url=f\"https://{ecosystem}.ecosystem-dashboard.com/events.json?range={number_of_days}&repo_full_name={org}%2F{repo}&\", output_path=events_path, filter=payload_filter, page_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read everything back in that was persisted to disk\n",
    "\n",
    "# report_date_str = \"2022-05-19\"\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for repo_config in repo_configs:\n",
    "    ecosystem = repo_config[\"ecosystem\"]\n",
    "    org = repo_config[\"org\"]\n",
    "    repo = repo_config[\"repo\"]\n",
    "    events_path = f\"{org}-{repo}-events-{report_date_str}.json\"\n",
    "    ecosystem_df = pd.read_json(events_path, orient='records')\n",
    "    ecosystem_df = ecosystem_df.set_index(\"github_id\")\n",
    "    df = pd.concat([df, ecosystem_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the event and action columns for analysis later.\n",
    "df[\"event/action\"] = df.apply(lambda x: x[\"event_type\"] + \"/\" + x[\"action\"] if x[\"action\"] else x[\"event_type\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the event/action based on what time of behavior it signals.\n",
    "\n",
    "event_action_classifications = {\n",
    "    \"IssuesEvent/opened\" : \"issue_engagement\",\n",
    "    \"IssueCommentEvent/created\" : \"issue_engagement\",\n",
    "    \"PullRequestReviewCommentEvent/created\" : \"code_review_engagement\",\n",
    "    \"PullRequestReviewEvent/created\" : \"code_review_engagement\",\n",
    "    \"PullRequestEvent/opened\" : \"code_creation\",\n",
    "    \"PushEvent\" : \"code_creation\",\n",
    "}\n",
    "df[\"event_action_classification\"] = df[\"event/action\"].map(event_action_classifications)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the column names.\n",
    "p1_col_names = [\n",
    "    'org',\n",
    "    'repository_full_name',\n",
    "    'actor',\n",
    "    'event/action',\n",
    "    \"event_action_classification\"\n",
    "]\n",
    "p2_col_names = df.columns.to_list()\n",
    "for p1_col_name in p1_col_names:\n",
    "    p2_col_names.remove(p1_col_name)\n",
    "\n",
    "ordered_col_names = []\n",
    "ordered_col_names.extend(p1_col_names)\n",
    "ordered_col_names.extend(p2_col_names)\n",
    "ordered_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ordered_col_names]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the number of \"actors\" for a given type of activity.\n",
    "\n",
    "# https://stackoverflow.com/questions/12860421/how-to-aggregate-unique-count-with-pandas-pivot-table\n",
    "table = pd.pivot_table(df, values='actor', index=[\"repository_full_name\", \"event_action_classification\"], aggfunc=pd.Series.nunique, fill_value=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect stats on PRs opened and closed\n",
    "\n",
    "table = pd.pivot_table(df.loc[df['event_type'] == \"PullRequestEvent\"], values='id', index=[\"repository_full_name\", \"event/action\"], aggfunc=\"count\", fill_value=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful debugging for what kind of event/actions are most popular.\n",
    "\n",
    "table = pd.pivot_table(df, values='id', index=['event/action'], aggfunc=\"count\", fill_value=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect how much activity indvidauls are having.\n",
    "table = pd.pivot_table(df, values='id', index=['actor'], aggfunc=\"count\", fill_value=0)\n",
    "table.sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the event data for additional analysis\n",
    "df.to_csv(f\"spec-github-activity-events-combined-cleaned-{report_date_str}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "You now have tabular data for each event, which makes it easy to create pivot tables to summarize how many actions a given user took."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
